{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import transfer_learning  # This module contains large amounts of pre-written code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure target log dir exists\n",
    "INCEPTION_LOG_DIR = './tmp/inception_v3_log'\n",
    "if not os.path.exists(INCEPTION_LOG_DIR):\n",
    "    os.makedirs(INCEPTION_LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking for images in 'AD'\n",
      "WARNING:tensorflow:WARNING: Folder AD has more than 150 images. Some images will never be selected.\n",
      "INFO:tensorflow:Looking for images in 'CN'\n",
      "WARNING:tensorflow:WARNING: Folder CN has more than 150 images. Some images will never be selected.\n",
      "INFO:tensorflow:Looking for images in 'EMCI'\n",
      "WARNING:tensorflow:WARNING: Folder EMCI has more than 150 images. Some images will never be selected.\n",
      "INFO:tensorflow:Looking for images in 'LMCI'\n",
      "WARNING:tensorflow:WARNING: Folder LMCI has more than 150 images. Some images will never be selected.\n"
     ]
    }
   ],
   "source": [
    "# For performance reasons, we only use 150 images per class.\n",
    "training_images, testing_images, label_maps = transfer_learning.create_image_lists(\n",
    "    './data/adni_photos', testing_percentage=10, max_number_images=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the inception model.\n",
    "graph, bottleneck, resized_input, softmax = transfer_learning.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a summary writer to write the loaded graph and display it in tensorboard.\n",
    "with graph.as_default():\n",
    "    jpeg_data, decoded_image = transfer_learning.make_jpeg_decoding()\n",
    "    summary_writer = tf.summary.FileWriter(os.path.join(INCEPTION_LOG_DIR, 'original'), graph)\n",
    "    summary_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Define summaries for tensorboard\n",
    "\n",
    "    output_summary = tf.summary.histogram('softmax_output', softmax)  # histogram of softmax output\n",
    "    input_summary = tf.summary.image('input_image', resized_input)  # display image of input\n",
    "    bottleneck_summary = tf.summary.histogram('bottleneck_activations', bottleneck)  # second to last layer output\n",
    "    #-- \n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_image(session, image_path, summary_op):\n",
    "    \n",
    "    # Open single image file and extract data\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_data = f.read()\n",
    "    \n",
    " \n",
    "    # Create Summary writer object\n",
    "    summary_writer = tf.summary.FileWriter(os.path.join(INCEPTION_LOG_DIR, 'original'), graph)\n",
    "        \n",
    "    # Run pre-defined pipeline to decode & resize raw image data into decoded_image \n",
    "    image = session.run(decoded_image, feed_dict={jpeg_data: image_data})\n",
    "        \n",
    "    # Run input through network and obtain output from last layer, defined by create_model() call above.\n",
    "\n",
    "    softmax_output, summary = session.run([softmax, summary_op], feed_dict={resized_input: image})\n",
    "    \n",
    "    # Log summary & terminate writer in the usual way.\n",
    "    summary_writer.add_summary(summary)\n",
    "    summary_writer.close()\n",
    "    #--\n",
    "    \n",
    "    # Return label\n",
    "    return(np.argmax(softmax_output),softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(os.getcwd(), 'data', 'adni_photos', 'AD', 'ADNI_130_S_4984_MR_MT1__N3m_Br_20121210160320615_S172131_I350508.nii-slice128.jpg')\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        # We classify the image and print the label\n",
    "        imagenet_label,_ = classify_image(session, image_path, summary_op)\n",
    "print(imagenet_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(os.getcwd(), 'data', 'adni_photos', 'AD', 'ADNI_130_S_4984_MR_MT1__N3m_Br_20121210160320615_S172131_I350508.nii-slice128.jpg')\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        imagenet_label,softmax_output = classify_image(session, image_path, summary_op)\n",
    "print(imagenet_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_bottleneck(session, image_data):\n",
    "\n",
    "    image = session.run(decoded_image, feed_dict={jpeg_data: image_data})\n",
    "    return session.run(bottleneck, feed_dict={resized_input: image})\n",
    "    #--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1/533 bottlenecks\n",
      "Saved 21/533 bottlenecks\n",
      "Saved 41/533 bottlenecks\n",
      "Saved 61/533 bottlenecks\n",
      "Saved 81/533 bottlenecks\n",
      "Saved 101/533 bottlenecks\n",
      "Saved 121/533 bottlenecks\n",
      "Saved 141/533 bottlenecks\n",
      "Saved 161/533 bottlenecks\n",
      "Saved 181/533 bottlenecks\n",
      "Saved 201/533 bottlenecks\n",
      "Saved 221/533 bottlenecks\n",
      "Saved 241/533 bottlenecks\n",
      "Saved 261/533 bottlenecks\n",
      "Saved 281/533 bottlenecks\n",
      "Saved 301/533 bottlenecks\n",
      "Saved 321/533 bottlenecks\n",
      "Saved 341/533 bottlenecks\n",
      "Saved 361/533 bottlenecks\n",
      "Saved 381/533 bottlenecks\n",
      "Saved 401/533 bottlenecks\n",
      "Saved 421/533 bottlenecks\n",
      "Saved 441/533 bottlenecks\n",
      "Saved 461/533 bottlenecks\n",
      "Saved 481/533 bottlenecks\n",
      "Saved 501/533 bottlenecks\n",
      "Saved 521/533 bottlenecks\n",
      "Done computing bottlenecks!\n"
     ]
    }
   ],
   "source": [
    "# This cell generates all the bottlenecks. Warning: it may take a while\n",
    "# The results are cached so you only need to do it once -- if you change\n",
    "# your compute_bottleneck function, you will need to delete the existing\n",
    "# files to force the notebook to regenerate the bottlenecks (they are\n",
    "# found in ./data/bottlenecks)\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        transfer_learning.cache_bottlenecks(compute_bottleneck, session, training_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This loads the training data as a matrix of training examples\n",
    "# and a vector of labels\n",
    "training_data_set = transfer_learning.create_training_dataset(training_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_final_layers(bottleneck_tensor, num_classes):\n",
    "   \n",
    "    bottleneck_tensor_size = int(bottleneck.shape[1])\n",
    "    \n",
    "    with tf.variable_scope('input'):\n",
    "        # This is the input for the bottleneck. It is created\n",
    "        # as a placeholder with default. During training, we will\n",
    "        # be passing in the bottlenecks, but during evaluation,\n",
    "        # the value will be propagated from the bottleneck computed\n",
    "        # from the image using the full network.\n",
    "        bottleneck_input = tf.placeholder_with_default(\n",
    "            bottleneck_tensor,\n",
    "            [None, bottleneck_tensor_size],\n",
    "            'bottleneck_input')\n",
    "        \n",
    "        # This is the input for the label (integer, 1 to number of classes)\n",
    "        label_input = tf.placeholder(tf.int64, [None], name='label_input')\n",
    "    \n",
    "    \n",
    "    # -- Start student must write\n",
    "    # Define weights, biases, and logit transforms\n",
    "    logits = tf.layers.dense(bottleneck_input, num_classes)\n",
    "    # Compute the cross entropy loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=label_input, logits=logits)\n",
    "    # Create a summary for the loss\n",
    "    loss_summary = tf.summary.scalar('cross_entropy', loss)\n",
    "    # Create a Gradient Descent Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    # Obtain a function which performs a single training step\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    # -- End student must write\n",
    "    \n",
    "    return bottleneck_input, label_input, logits, train_step, loss_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(labels, logits):\n",
    "    \n",
    "    \n",
    "    # Collapse logit output to scalar labels\n",
    "    predicted_label = tf.argmax(logits, 1)\n",
    "    # Compute accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_label, labels), tf.float32))\n",
    "    # create summary of accuracy\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "    #--\n",
    "    \n",
    "    return accuracy, accuracy_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create the necessary operations to fine tune the model.\n",
    "\n",
    "with graph.as_default():\n",
    "    bottleneck_input, label_input, logits, train_step, loss_summary = make_final_layers(bottleneck, len(label_maps))\n",
    "    accuracy, accuracy_summary = compute_accuracy(label_input, logits)\n",
    "    summary_op = tf.summary.merge([loss_summary, accuracy_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_train_step(session: tf.Session, summary_writer: tf.summary.FileWriter, current_step: int):\n",
    "\n",
    "    _, ac, summary = session.run((train_step, accuracy, summary_op),\n",
    "                       feed_dict={bottleneck_input: training_data_set['bottlenecks'],\n",
    "                                  label_input: training_data_set['labels']\n",
    "                                 })\n",
    "    \n",
    "    summary_writer.add_summary(summary, current_step)\n",
    "    \n",
    "    if current_step % 10 == 0:\n",
    "        print('Accuracy at step {0} is {1}'.format(current_step, ac))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_images(session: tf.Session, images_jpeg_data: [bytes], labels: [int]):\n",
    "  \n",
    "    correct = []\n",
    "    \n",
    "    for label, jpeg in zip(labels, images_jpeg_data):\n",
    "        image_data = session.run(decoded_image, feed_dict={jpeg_data: jpeg})\n",
    "        ac = session.run(accuracy, feed_dict={resized_input: image_data, label_input: [label]})\n",
    "        correct.append(ac)\n",
    "    \n",
    "    return np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Starting training ----------------\n",
      "Accuracy at step 0 is 0.5910780429840088\n",
      "Accuracy at step 10 is 0.48327139019966125\n",
      "Accuracy at step 20 is 0.5315985083580017\n",
      "Accuracy at step 30 is 0.643122673034668\n",
      "Accuracy at step 40 is 0.5464683771133423\n",
      "Accuracy at step 50 is 0.6579925417900085\n",
      "Accuracy at step 60 is 0.6356877088546753\n",
      "Accuracy at step 70 is 0.5687732100486755\n",
      "Accuracy at step 80 is 0.643122673034668\n",
      "Accuracy at step 90 is 0.654275119304657\n",
      "Accuracy at step 100 is 0.5836431384086609\n",
      "Accuracy at step 110 is 0.6617100238800049\n",
      "Accuracy at step 120 is 0.6579925417900085\n",
      "Accuracy at step 130 is 0.535315990447998\n",
      "Accuracy at step 140 is 0.7063196897506714\n",
      "Accuracy at step 150 is 0.5910780429840088\n",
      "Accuracy at step 160 is 0.6914498209953308\n",
      "Accuracy at step 170 is 0.6691449880599976\n",
      "Accuracy at step 180 is 0.5576208233833313\n",
      "Accuracy at step 190 is 0.7137546539306641\n",
      "------------- Training done! -------------------\n",
      "---------- Loading testing data ----------------\n",
      "----------- Evaluating on testing --------------\n",
      "Evaluation accuracy was: 0.6153846383094788\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        print('------------- Starting training ----------------')\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(os.path.join(INCEPTION_LOG_DIR, 'retrained'), graph)\n",
    "        for i in range(200):\n",
    "            execute_train_step(session, summary_writer, i)\n",
    "        summary_writer.close()  \n",
    "        print('------------- Training done! -------------------')\n",
    "        print('---------- Loading testing data ----------------')\n",
    "        tlabels, timages = transfer_learning.get_testing_data(testing_images)\n",
    "        print('----------- Evaluating on testing --------------')\n",
    "        \n",
    "        eval_accuracy = evaluate_images(session, timages, tlabels)\n",
    "        print('Evaluation accuracy was: {0}'.format(eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
